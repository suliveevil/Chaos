---
title: "分词"
categories: 
date: 2022-10-28 23:35:03 +0800
tags: 
excerpt: 
---


## 分词工具

[fxsjy/jieba](https://github.com/fxsjy/jieba): 结巴[[中文]]分词

LTP - 哈尔滨工业大学

ICTCLAS - 北京理工大学

THULAC - 清华大学

[[HanLP]]

[ownthink/Jiagu](https://github.com/ownthink/Jiagu): Jiagu深度学习自然语言处理工具 知识图谱关系抽取 中文分词 词性标注 命名实体识别 情感分析 新词发现 关键词 文本摘要 文本聚类

[SeanLee97/xmnlp: xmnlp](https://github.com/SeanLee97/xmnlp)：轻量级[[中文]][[自然语言处理]]工具。提供中文分词, 词性标注, 命名体识别，情感分析，文本纠错，文本转拼音，文本摘要，偏旁部首，句子表征及[[文本相似度]]计算等功能

[Moonshile/ChineseWordSegmentation:](https://github.com/Moonshile/ChineseWordSegmentation) Chinese word segmentation algorithm without corpus（无需语料库的中文分词）

[hankcs/pyhanlp](https://github.com/hankcs/pyhanlp): 中文分词。配套书籍：[《自然语言处理入门》](http://nlp.hankcs.com/book.php)

[ownthink/Jiagu](https://github.com/ownthink/Jiagu): Jiagu 深度学习[[自然语言处理]]工具 知识图谱关系抽取 中文分词 词性标注 命名实体识别 情感分析 新词发现 关键词 文本摘要 文本聚类

[yishn/chinese-tokenizer](https://github.com/yishn/chinese-tokenizer): Tokenizes Chinese texts into words.

## App 分词

[[Vim]]：iskeyword 设置分词

[[Emacs]] 中文分词高亮：[[cnhl]]


## Misc


[互联网时代的社会语言学：基于SNS的文本数据挖掘 | Matrix67: The Aha Moments](http://www.matrix67.com/blog/archives/5044)





